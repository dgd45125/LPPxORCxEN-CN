`compute_english_regressors_github.ipynb` uses the F0 (`*_f0.csv`), word-by-word log lexical frequency (`*_freq.csv`), root mean squared amplitude of the spoken narration (`*_RMS.csv`), word-by-word GPT2 surprisal value (`Prince_gpt2.tsv`), word-by-word syntactic complexity metric (`disc_proj_en_10jan2022.csv`), and word-by-word long-distance dependency annotation (`COMPLEXITIES__en_9apr2022.csv`) source materials to prepare the English regressors.
  - `*_f0.csv`, `*_freq.csv`, and `*_RMS.csv` are chopped up spreadsheets from the annotations provided in the <a href="https://openneuro.org/datasets/ds003643/versions/2.0.1">Le Petit Prince OpenNEURO repository</a>.
  - `Prince_gpt2.tsv` comes from feeding `Prince.txt` into <a href="https://cpllab.github.io/lm-zoo/">lm-zoo</a> and getting the GPT2 surprisal values.
  - The word-by-word syntactic complexity metric from `disc_proj_en_10jan2022.csv` and the long-distance dependency annotations from `COMPLEXITIES__en_9apr2022.csv` were created via Deepmind-internal code which cannot be released. The process, however, is described in the paper.
