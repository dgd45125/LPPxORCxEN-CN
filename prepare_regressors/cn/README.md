- `compute_chinese_regressors_github.ipynb` uses the prosody (`annotaion_CN_lppCN_prosody.csv`), word-by-word Chinese GPT2 surprisal value (`lpp_CN_word_surprisal_gpt2.csv`), and word-by-word syntactic complexity and long-distance dependency annotation (`COMPLEXITIES__ch_9apr2022.csv`) source materials to prepare the Chinese regressors.
  - `annotation_CN_lppCN_prosody.csv`comes from the <a href="https://openneuro.org/datasets/ds003643/versions/2.0.1">Le Petit Prince OpenNEURO repository</a>.
  - `lpp_CN_word_surprisal_gpt2.csv` comes from the `lpp_gpt2_CN_surprisal.py` script which uses `lpp_CN_new_sentence.csv` as the input chinese text and `lppCN_word_information_sent_info.csv` for word segmentation.
  - The word-by-word syntactic complexity metric and long-distance dependency annotations from `COMPLEXITIES__ch_9apr2022.csv` were created via Deepmind-internal code which cannot be released. The process, however, is described in the paper.
